<!DOCTYPE html>
<html>
  <head>
  	<title>EWHA HCIL</title>
    <meta charset="utf-8"> 
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> 
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="../css/default.css">
    <link rel="stylesheet" type="text/css" href="../css/projectStyle.css">
    <link rel="stylesheet" type="text/css" href="../css/topButtonStyle.css">
  </head>
  <body>
    <svg onclick="topFunction()" title="Go to top" id="topBtn" width="3em" height="3em" viewBox="0 0 16 16" class="bi bi-arrow-up-circle-fill" fill="#408053" xmlns="http://www.w3.org/2000/svg">
      <path fill-rule="evenodd" d="M16 8A8 8 0 1 1 0 8a8 8 0 0 1 16 0zm-7.5 3.5a.5.5 0 0 1-1 0V5.707L5.354 7.854a.5.5 0 1 1-.708-.708l3-3a.5.5 0 0 1 .708 0l3 3a.5.5 0 0 1-.708.708L8.5 5.707V11.5z"/>
    </svg> 

  	<nav class="navbar navbar-expand-lg navbar-dark bg-dark fixed-top">
      <div class="container">
        <a href="../index.html"><img src="../img/menu/homepageLogo_fin2.png" class="navbar-brand"></a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbars" aria-controls="navbars" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbars">
          <ul class="navbar-nav mr-auto">
          	<li class="nav-item">
              <a class="nav-link" id="menu-link" href="../index.html">Home <span class="sr-only">(current)</span></a>
            </li>
            <li class="nav-item">
              <a class="nav-link" id="menu-link" href="./people.html">People</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" id="menu-link" href="./project.html">Projects</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" id="menu-link" href="./publish.html">Publications</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" id="menu-link" href="./news.html">News</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" id="menu-link" href="./contact.html">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>
  	
    <section>
      <div class="container">
        <h1 class="menu_title">Projects</h1>
      </div>
    </section>

    <section>
      <article>
        <div class="container" id="proj_PERSON">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/Personal Object Recognizer.jpg" class="card-img" alt="...">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">PERSONAL OBJECT RECOGNIZER FOR PEOPLE WITH VISUAL IMPAIRMENTS</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p class="card-text">Blind people often need to identify objects around them, from packages of food to items of clothing. Automatic object recognition continues to provide limited assistance in such tasks because models tend to be trained on images taken by sighted people with different background clutter, scale, viewpoints, occlusion, and image quality than in photos taken by blind users.</p>
              <h5 class="sub_title">Related Publication(s)</h5>
              <p class="card-text">Kacorri, H., Kitani, K. M., Bigham, J. P., & Asakawa, C. (2017, May). People with Visual Impairment Training Personal Object Recognizers: Feasibility and Challenges. CHI'17 (pp. 5839-5849). ACM.
                <a class="pdf_link" href="http://www.andrew.cmu.edu/user/hkacorri/pub/2017-chi-kacorri_et_al.pdf">[PDF]</a>
              </p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container" id="proj_IMPROV">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/IMPROVING SELFIE EXPERIENCES FOR BLIND PEOPLE.jpg" class="card-img" alt="...">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">IMPROVING SELFIE EXPERIENCES FOR BLIND PEOPLE</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p class="card-text">Selfies have been a major social trend for many years. However, it can be challenging for people with visual impairments to take part in this activity although their use of social media is as high as sighted people. Thus, we designed and developed a mobile application for helping people with visual impairments with taking and managing selfies.</p>
              <h5 class="sub_title">Related Publication(s)</h5>
              <p class="card-text">Yunjung Lee, hajung Kim, Hyeji Jang, Yujin Han, Uran Oh. (2019) Selfer: Selfie Guidance Mobile Application for the Blind. Proceedings of HCI Korea 2019. 971-975</p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container" id="proj_YOLO">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/YOLO-BASED WALKING ASSISTANCE FOR BLIND PEOPLE.jpg" alt="..." class="card-img">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">YOLO-BASED WALKING ASSISTANCE FOR BLIND PEOPLE</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p>People with visual impairments walk along braille blocks. In particular, viscous blocks help to stop walking at a point where it can be dangerous. However, many Braille blocks on Korean roads are often damaged or absent. Thus, we have implemented a mobile walking assistance app for people with visual impairments, which informs users' current location, sidewalk location, bus info with verbal feedback.</p>
              <h5 class="sub_title">Related Publication(s)</h5>
              <p>Jiwon Yoo, DongHee Han, Chayoung Hur, Uran Oh. (2019) YOLO-based Walking Assistance Application for Blind People. Korea Computer Congress 2019. Participation Award for Student Paper Competition</p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container" id="proj_NAVCOG">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/NAVCOG_INDOOR NAVIGATION ASSISTANCE FOR PEOPLE WITH VISUAL IMPAIRMENTS.png" class="card-img">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">NAVCOG: INDOOR NAVIGATION ASSISTANCE FOR PEOPLE WITH VISUAL IMPAIRMENTS</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p>When in an unfamiliar place, people tend to use a walking navigation system on their device to compare the map location to the surrounding views. However, visually impaired people cannot check the map or the surrounding scenery to bridge the gap between the ground truth and the rough GPS location. NavCog aims for an improved high-accuracy walking navigation system that uses BLE beacons together with various kinds of sensors with a new localization algorithm for both indoors and outdoors.
                <a class="pdf_link" href="http://www.cs.cmu.edu/~NavCog/navcog.html">[A link to the project]</a>
              </p>
              <h5 class="sub_title">Related Publication(s)</h5>
              <p>Ahmetovic, D., Gleason, C., Ruan, C., Kitani, K., Takagi, H., & Asakawa, C. (2016, September). NavCog: a navigational cognitive assistant for the blind. MobileHCI'16 (pp. 90-99). ACM.
                <a class="pdf_link" href="https://www.researchgate.net/profile/Dragan_Ahmetovic/publication/306255539_NavCog_A_Navigational_Cognitive_Assistant_for_the_Blind/links/57b4dc7308aeaab2a1039e54.pdf">[PDF]</a>
              </p>
              <p>NavCog3: An Evaluation of a Smartphone-Based Blind Indoor Navigation Assistant with Semantic Features in a Large-Scale Environment
                <a class="pdf_link" href="http://www.uranoh.com/publications/ASSETS17-navcog.pdf">[PDF]</a>
              </p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/NONVISUAL ON-BODY INTERACTION.png" class="card-img">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">NONVISUAL ON-BODY INTERACTION</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p>For users with visual impairments, who do not necessarily need the visual display of a mobile device, non-visual on-body interaction (e.g., Imaginary Interfaces) could provide accessible input in a mobile context. Such interaction provides the potential advantages of an always-available input surface, and increased tactile and proprioceptive feedback compared to a smooth touchscreen.</p>
              <h5 class="sub_title">Related Publication(s)</h5>
              <p>Design of and Subjective Response to on-Body Input for People With Visual Impairments
                <a class="pdf_link" href="http://www.uranoh.com/publications/ASSETS2014-oh.pdf">[PDF]</a>
              </p>
              <p>A Performance Comparison of on-Hand Versus on-Phone Nonvisual Input by Blind and Sighted Users
                <a class="pdf_link" href="http://www.uranoh.com/publications/TACCESS2015-oh_onhand.pdf">[PDF]</a>
              </p>
              <p>Localization of Skin Features on the Hand and Wrist from Small Image Patches
                <a class="pdf_link" href="http://www.uranoh.com/publications/ICPR2016-stearns.pdf">[PDF]</a>
              </p>
              <p>Investigating Microinteractions for People with Visual Impairments and the Potential Role of On-Body Interaction
                <a class="pdf_link" href="http://www.uranoh.com/publications/ASSETS2017-oh.pdf">[PDF]</a>
              </p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/READING ASSISTANCE VIA A FINGER-MOUNTED DEVICE.png" class="card-img">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">READING ASSISTANCE VIA A FINGER-MOUNTED DEVICE</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p>The recent miniaturization of cameras has enabled finger-based reading approaches that provide blind and visually impaired readers with access to printed materials. Compared to handheld text scanners such as mobile phone applications, mounting a tiny camera on the user’s own finger has the potential to mitigate camera framing issues, enable a blind reader to better understand the spatial layout of a document, and provide better control over reading pace.</p>
              <h5 class="sub_title">Related Publication(s)</h5>
              <p>The Design and Preliminary Evaluation of Finger-Mounted Camera and Feedback System to Enable Reading of Printed Text for the Blind
                <a class="pdf_link" href="http://uranoh.com/publications/ACVR2014-stearns.pdf">[PDF]</a>
              </p>
              <p>Evaluating Haptic and Auditory Directional Guidance to Assist Blind People in Reading Printed Text Using Finger-Mounted Cameras
                <a class="pdf_link" href="http://www.uranoh.com/publications/TACCESS2016-stearns.pdf">[PDF]</a>
              </p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/MOBILE AND WEARABLE DEVICE USE FOR PEOPLE WITH VISUAL IMPAIRMENTS.png" class="card-img">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">MOBILE AND WEARABLE DEVICE USE FOR PEOPLE WITH VISUAL IMPAIRMENTS</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p>With the increasing popularity of mainstream wearable devices, it is critical to assess the accessibility implications of such technologies. For people with visual impairments, who do not always need the visual display of a mobile phone, alternative means of eyes-free wearable interaction are particularly appealing.</p>
              <h5 class="sub_title">Related Publication(s)</h5>
              <p>Current and Future Mobile and Wearable Device Use by People With Visual Impairments
                <a class="pdf_link" href="http://www.uranoh.com/publications/CHI2014-ye.pdf">[PDF]</a>
              </p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/TOUCHSCREEN GESTURES SONIFICATION.png" class="card-img">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">TOUCHSCREEN GESTURES SONIFICATION</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p>While sighted users may learn to perform touchscreen gestures through observation (e.g., of other users or video tutorials), such mechanisms are inaccessible for users with visual impairments. As a result, learning to perform gestures without visual feedback can be challenging.</p>
              <h5 class="sub_title">Related Publication(s)</h5>
              <p>Follow That Sound: Using Sonification and Corrective Verbal Feedback to Teach Touchscreen Gestures
                <a class="pdf_link" href="http://www.uranoh.com/publications/ASSETS2013-oh.pdf">[PDF]</a>
              </p>
              <p>Audio-Based Feedback Techniques for Teaching Touchscreen Gestures
                <a class="pdf_link" href="http://www.uranoh.com/publications/TACCESS2015-oh_sonification.pdf">[PDF]</a>
              </p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/END-USER TOUCHSCREEN GESTUERS CUSTOMIZATION.png" class="card-img">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">END-USER TOUCHSCREEN GESTUERS CUSTOMIZATION</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p>The vast majority of work on understanding and supporting the gesture creation process has focused on professional designers. In contrast, gesture customization by end users— which may offer better memorability, efficiency and accessibility than pre-defined gestures—has received little attention.</p>
              <h5 class="sub_title">Related Publication(s)</h5>
              <p>The Challenges and Potential of End-User Gesture Customization
                <a class="pdf_link" href="http://www.uranoh.com/publications/CHI2013-oh.pdf">[PDF]</a>
              </p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/NONVISUAL GUIDANCE FOR ASSISTING SPATIAL TASKS IN 3D SPACE WITH SIX DEGREES OF FREEDOM.png" class="card-img">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">NONVISUAL GUIDANCE FOR ASSISTING SPATIAL TASKS IN 3D SPACE WITH SIX DEGREES OF FREEDOM</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p>With the advancement of technologies in augmented or virtual reality (AR/VR) research, the interaction such as navigation or object manipulation is no longer limited to two dimension. However, the assitance for supporting interactions in 3-dimensional space for people with visual impairments has not been well-explored, especially when the degree of freedom is high to be delievered to users at once.</p>
              <h5 class="sub_title">The Goal and Expected Contributions</h5>
              <p>Designing and implementing a system that conveys spatial information of the surrondings in 3D space with nonvisual feedback with minimum cognitive loads. This system may be used for assisting object localization (i.e., helping a blind person to reach to a specific object) and photography for people with visual impairments.</p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/SUPPORTING INSTANCE NAVIGATION OF PHOTOS FOR PEOPLE WITH VISUAL IMPAIRMENTS .png" class="card-img">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">SUPPORTING INSTANCE NAVIGATION OF PHOTOS FOR PEOPLE WITH VISUAL IMPAIRMENTS</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p>Object classification/localization or scene summarization has been an on-going research topic in computer vision for many years. While these can benefit people with visual impairments to have better access to visual contents of an image, it is still challenging for them to fully understand the scene, which may prevents many of them from being socially engaged with friends
                   <a class="pdf_link" href="https://research.fb.com/wp-content/uploads/2017/02/aat_cscw2017_camera_ready_20161031-2.pdf"> [ref].</a> </p>
              <h5 class="sub_title">The Goal and Expected Contributions</h5>
              <p>Developing a system that enables users with visual impairments to gain better understanding of a complex scene with multiple instance by allowing users to spatially explore each instance in the scene by touch. The system can improve the accessibility of images by providing visual information of images in detail.</p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/EYES-FREE TEXT ENTRY WITH WEARABLE SENSORS.jpg" class="card-img">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">EYES-FREE TEXT ENTRY WITH WEARABLE SENSORS</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p>Text-entry can sometimes be not always-available or efficient, especially in mobile contexts where users have to constantly monitor their surrondings for their safety (e.g., avoid bumping into people or obstacles).</p>
              <h5 class="sub_title">The Goal and Expected Contributions</h5>
              <p>Designing and implementiong a wearable device with finger-, or wrist-mounted sensors such as inertial motion unit or electromyography (EMG) sensors. This device should enables users to enter texts without visual feedback.</p>
            </div>
          </div>
        </div>
      </article>
      <article>
        <div class="container">
          <div class="row">
            <div class="col-sm-3">
              <img src="../img/project/ADAPTIVE APP LAUNCHER FOR SMARTWATCHES.png" class="card-img">
            </div>
            <div class="col-sm-9">
              <h4 class="project_title">ADAPTIVE APP LAUNCHER FOR SMARTWATCHES</h4>
              <h5 class="sub_title">The Motivation</h5>
              <p>Selecting a target from a collection of items on a smartwatch is a frequent yet challenging task. The constrained screen real estate can only accommodate a small number of items to be large enough for finger tips. As a result, users often need to search through a long list of items or navigate UI hierarchies to find and select a target.</p>
              <h5 class="sub_title">The Goal and Expected Contributions</h5>
              <p>Developing an app launcher for a smartwatch which adaptively changes the layout of the apps or the size of the app icons based their app launch likelihoods. This would enable users to find and open a desired app with efficiency.</p>
            </div>
          </div>
        </div>
      </article>
    </section>

    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"> </script> 
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"> </script> 
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"> </script>
    <script src="../js/topButton.js"></script>
  </body>
</html>